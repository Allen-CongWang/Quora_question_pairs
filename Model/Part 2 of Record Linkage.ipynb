{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Quora question pairs","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\n\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the training data\nquora_train = pd.read_csv('/kaggle/input/quora-question/quora_train.csv')\nquora_train = quora_train.drop('Unnamed: 0', axis = 1)\nquora_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the test data\nquora_test = pd.read_csv('/kaggle/input/quora-question/quora_test.csv')\nquora_test = quora_test.drop('Unnamed: 0', axis = 1)\nquora_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the rows that have missing values\nquora_train = quora_train.dropna(axis = 0).reset_index(drop = True)\nquora_test = quora_test.dropna(axis = 0).reset_index(drop = True)\n\nquora_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_train['question1'] = quora_train['question1'].astype('str')\nquora_train['question2'] = quora_train['question2'].astype('str')\nquora_test['question1'] = quora_test['question1'].astype('str')\nquora_test['question2'] = quora_test['question2'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_train['question1'] = quora_train['question1'].apply(lambda s: text_to_wordlist(s))\nquora_train['question2'] = quora_train['question2'].apply(lambda s: text_to_wordlist(s))\nquora_test['question1'] = quora_test['question1'].apply(lambda s: text_to_wordlist(s))\nquora_test['question2'] = quora_test['question2'].apply(lambda s: text_to_wordlist(s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data explanatory analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number of duplicate and distinct question pairs\nquora_train['is_duplicate'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of questions that occur more than once\nquestion_count = pd.concat([quora_train['qid1'], quora_train['qid2']]).value_counts().values\nprint('There are ' + str(sum(question_count > 1)) + ' repeated questions.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(12, 5))\n\n# Distribution of the times each question occurs\nplt.hist(question_count, bins=50)\n\n# Take logarithm of y-value\nplt.yscale('log', nonposy='clip')\n\nplt.title('Log-Histogram of question appearance counts')\n\n# Set axis labels\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The length of queation1 and question2\nsizeQuestionOne = quora_train['question1'].str.len().tolist()\nsizeQuestionTwo = quora_train['question2'].str.len().tolist()\n\nmin(sizeQuestionOne), min(sizeQuestionTwo), max(sizeQuestionOne), max(sizeQuestionTwo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(8, 6))\n\n# Distribution of the length of question 1\nplt.hist(sizeQuestionOne, bins=100, range=[0, 300])\n\nplt.title('Histogram of character count in questions', fontsize=13)\n\n# Set axis labels\nplt.xlabel('Number of characters', fontsize=13)\nplt.ylabel('Number of questions', fontsize=13)\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(8, 6))\n\n# Distribution of the length of question 2\nplt.hist(sizeQuestionTwo,bins=100, range=[0, 300])\n\nplt.title('Histogram of character count in questions', fontsize=13)\n\n# Set axis labels\nplt.xlabel('Number of characters', fontsize=13)\nplt.ylabel('Number of questions', fontsize=13)\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(8, 6))\n\n# Relationship between the length of question 1 and question 2\nplt.scatter(sizeQuestionOne, sizeQuestionTwo)\n\nplt.title('Correlation of character count in question1 and question2', fontsize=13)\n\n# Set axis labels\nplt.xlabel('Number of characters of question1', fontsize=13)\nplt.ylabel('Number of characters of question2', fontsize=13)\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number of words in question 1 and question 2\nwordQuestionOne = quora_train['question1'].apply(lambda s: len(s.split(\" \")))\nwordQuestionTwo = quora_train['question2'].apply(lambda s: len(s.split(\" \")))\n\nmin(wordQuestionOne), min(wordQuestionTwo), max(wordQuestionOne), max(wordQuestionTwo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(8, 6))\n\n# Distribution of the number of words in question 1\nplt.hist(wordQuestionOne, bins=100, range=[0, 60])\n\nplt.title('Histogram of word count in questions', fontsize=13)\n\n# Set axis labels\nplt.xlabel('Number of words', fontsize=13)\nplt.ylabel('Number of questions', fontsize=13)\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(8, 6))\n\n# Distribution of the number of words in question 2\nplt.hist(wordQuestionTwo, bins=100, range=[0, 60])\n\nplt.title('Histogram of word count in questions', fontsize=13)\n\n# Set axis labels\nplt.xlabel('Number of words', fontsize=13)\nplt.ylabel('Number of questions', fontsize=13)\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(8, 6))\n\n# Relationship between the number of words in question1 and question2\nplt.scatter(wordQuestionOne, wordQuestionTwo)\n\nplt.title('Correlation of word count in question1 and question2', fontsize=13)\n\n# Set axis labels\nplt.xlabel('Number of words of question1', fontsize=13)\nplt.ylabel('Number of words of question2', fontsize=13)\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_word_share(row):\n    '''\n    Compute the proportion of the same words in two texts\n    '''\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n\nwordShare = quora_train.apply(normalized_word_share, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size of the plot\nplt.figure(figsize=(14, 8))\n\nplt.subplot(1,2,1)\n\n# Distribution of word share for duplicate pairs and distinct pairs\nsns.violinplot(x = quora_train['is_duplicate'], y = wordShare)\n\nplt.title('Percentage of word in common between question1 and question2', fontsize=13)\n\n# Set axis labels\nplt.xlabel('is_duplicate', fontsize=13)\nplt.ylabel('Percentage of word share', fontsize=13)\n\n\nplt.subplot(1,2,2)\nsns.distplot(wordShare[quora_train['is_duplicate'] == 1.0], color = 'green', label = 'is_duplicate = 1')\nsns.distplot(wordShare[quora_train['is_duplicate'] == 0.0], color = 'red', label = 'is_duplicate = 0')\n\nplt.title('Distribution of word share between question1 and question2', fontsize=13)\n\n# Set axis labels\nplt.xlabel('Percentage of word share', fontsize=13)\nplt.ylabel('Probability', fontsize=13)\n\nplt.legend()\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nMAX_SEQUENCE_LENGTH = 30 # Only the first 30 words in a text are taken into account\nMAX_NB_WORDS = 200000 # Only the most common 200,000 words are to be tokenized\nEMBEDDING_DIM = 300 # Dimension of embeddings\n\nnum_lstm = 205 # The output dimensionality of the LSTM layer\nnum_dense = 125 # The number of hidden units of the Dense layer\nrate_drop_dense = 0.15 # The dropout rate of the Dropout layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize the words in all questions\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(quora_train['question1'].\\\n                       append([quora_train['question2'], quora_test['question1'], quora_test['question2']]))\n\nsequences_1 = tokenizer.texts_to_sequences(quora_train['question1'])\nsequences_2 = tokenizer.texts_to_sequences(quora_train['question2'])\ntest_sequences_1 = tokenizer.texts_to_sequences(quora_test['question1'])\ntest_sequences_2 = tokenizer.texts_to_sequences(quora_test['question2'])\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\ndata_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = np.array(quora_train['is_duplicate'])\nprint('Shape of data tensor:', data_1.shape)\nprint('Shape of label tensor:', labels.shape)\n\ntest_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Index word vectors\nword2vec = KeyedVectors.load_word2vec_format('/kaggle/input/embedding/GoogleNews-vectors-negative300.bin', binary=True)\nprint('Found %s word vectors of word2vec' % len(word2vec.vocab))\n\n# Prepare embeddings\nprint('Preparing embedding matrix')\n\nnb_words = min(MAX_NB_WORDS, len(word_index))+1\n\nembedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if word in word2vec.vocab:\n        embedding_matrix[i] = word2vec.word_vec(word)\nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the model structure\nembedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],\\\n                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n\nlstm_layer = LSTM(num_lstm)\n\nsequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences_1 = embedding_layer(sequence_1_input)\nx1 = lstm_layer(embedded_sequences_1)\n\nsequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences_2 = embedding_layer(sequence_2_input)\ny1 = lstm_layer(embedded_sequences_2)\n\nmerged = concatenate([x1, y1])\nmerged = Dropout(rate_drop_dense)(merged)\nmerged = BatchNormalization()(merged)\n\nmerged = Dense(num_dense, activation= 'relu')(merged)\nmerged = Dropout(rate_drop_dense)(merged)\nmerged = BatchNormalization()(merged)\n\npreds = Dense(1, activation='sigmoid')(merged)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample train/validation data\nVALIDATION_SPLIT = 0.2 # The proportion of validation data\nperm = np.random.permutation(len(data_1)) # Shuffle the index\nidx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\nidx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n\ndata_1_train = data_1[idx_train]\ndata_2_train = data_2[idx_train]\nlabels_train = labels[idx_train]\n\ndata_1_val = data_1[idx_val]\ndata_2_val = data_2[idx_val]\nlabels_val = labels[idx_val]\n\n# Whether to re-weight classes to fit the 17.5% share in test set\nre_weight = True\n\n# Add class weight\nif re_weight:\n    class_weight = {0: 1.309028344, 1: 0.472001959}\nelse:\n    class_weight = None\n\nweight_val = np.ones(len(labels_val))\nif re_weight:\n    weight_val *= 0.472001959\n    weight_val[labels_val==0] = 1.309028344","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nmodel = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\nmodel.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n\nhist = model.fit([data_1_train, data_2_train], labels_train, \\\n                 validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n                 epochs=10, batch_size=128, shuffle=True, \\\n                 class_weight=class_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The accuracy of the training data and validation data\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']\n\n# Loss of the training data and validation data\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\n\n# Set axis labels\nplt.xlabel('epoch', fontsize=13)\nplt.ylabel('accuracy', fontsize=13)\n\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\n\n# Set axis labels\nplt.xlabel('epoch', fontsize=13)\nplt.ylabel('accuracy', fontsize=13)\n\nplt.title('Training and validation loss')\n\nplt.legend()\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the model on test data\nmodel.evaluate(x = [test_data_1, test_data_2], y = quora_test['is_duplicate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_test['prob'] = model.predict([test_data_1, test_data_2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Precision and recall\nnn_precision, nn_recall, _ = precision_recall_curve(quora_test['is_duplicate'], quora_test['prob'])\n\n# f-value\nnn_f = 2*nn_precision*nn_recall / (nn_precision+nn_recall)\n\n# Plot precision-recall curve\nplt.plot(nn_recall, nn_precision, marker='.')\n\n# Set axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('The precision-recall curve')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The precision, recall and f-value curve\nplt.plot(_, nn_precision[:-1], label = 'precision')\nplt.plot(_, nn_recall[:-1], label = 'recall')\nplt.plot(_, nn_f[:-1], label = 'f-value')\n\nplt.legend(loc = 'lower center')\n\n# Set axis label\nplt.xlabel('threshold')\n\nplt.title('The precision/ recall/ f-value curve')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_test['pred'] = 0\nquora_test.loc[quora_test['prob'] > 0.38, 'pred'] = 1 # The predicted label\n\nquora_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    \n    B =(C/C.sum(axis=0))\n\n    plt.figure(figsize=(20,4))\n    \n    labels = [0,1]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()\n    \nplot_confusion_matrix(quora_test['is_duplicate'], quora_test['pred'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the accuracy, precision, recall, and f-value given the threshold\nprint(accuracy_score(quora_test['is_duplicate'], quora_test['pred']))\nprint(precision_score(quora_test['is_duplicate'], quora_test['pred']))\nprint(recall_score(quora_test['is_duplicate'], quora_test['pred']))\nprint(f1_score(quora_test['is_duplicate'], quora_test['pred']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_fpr, nn_tpr, _ = roc_curve(quora_test['is_duplicate'], quora_test['prob'])\n\n# Plot ROC curve\nplt.plot(nn_fpr, nn_tpr, marker='.')\n\n# Set axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('The ROC curve')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute AUC\nnn_auc = roc_auc_score(quora_test['is_duplicate'], quora_test['prob'])\nprint('ROC AUC=%.3f' % (nn_auc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}